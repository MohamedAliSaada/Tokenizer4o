# -*- coding: utf-8 -*-
"""Tokenizer4o.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_eX4a8GxrGRpMlT_l8-5VxySVylegqA
"""

import os
import regex as re
import json
import base64
from collections import defaultdict
from typing import Dict, List, Tuple, Set

class Tokenizer4o:
    GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

    def __init__(self, special_tokens: Dict[str, int] = None):
        self.special_tokens = special_tokens or {}
        self.inverse_special = {v: k for k, v in self.special_tokens.items()}
        self._validate_special_tokens()

        self.pattern = re.compile(self.GPT4_SPLIT_PATTERN)
        self.vocab = self._init_base_vocab()
        self.merges: Dict[Tuple[int, int], int] = {}
        self.merge_order: List[Tuple[int, int]] = []
        self._reserved_ids = set(self.special_tokens.values())

    def _init_base_vocab(self) -> Dict[int, bytes]:
        return {i: bytes([i]) for i in range(256)}

    def _validate_special_tokens(self):
        for token, idx in self.special_tokens.items():
            if not isinstance(token, str):
                raise TypeError(f"Special token must be string: {token!r}")
            if idx < 256:
                raise ValueError(f"Special token ID {idx} < 256 conflicts with byte values")

    def train(self, text: str, vocab_size: int):
        if vocab_size < 256 + len(self.special_tokens) + 1:
            required = 256 + len(self.special_tokens) + 1
            raise ValueError(f"Vocab size too small. Needs at least {required}")

        text = self._remove_special_tokens(text)
        chunks = re.findall(self.pattern, text)
        if not chunks:
            raise ValueError("No text available for training after removing special tokens")

        ids_list = [list(chunk.encode('utf-8')) for chunk in chunks]
        max_merges = vocab_size - 256 - len(self.special_tokens)
        self.merge_order = []

        for _ in range(max_merges):
            stats = defaultdict(int)
            for ids in ids_list:
                for pair in zip(ids, ids[1:]):
                    stats[pair] += 1

            if not stats:
                break

            best_pair = max(stats, key=lambda x: (stats[x], x))
            merge_id = 256 + len(self.merges)

            if merge_id in self._reserved_ids:
                raise ValueError(f"Merge ID {merge_id} conflicts with special token IDs")

            self.merges[best_pair] = merge_id
            self.merge_order.append(best_pair)
            self.vocab[merge_id] = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]
            ids_list = [self._apply_merge(ids, best_pair, merge_id) for ids in ids_list]

    def encode(self, text: str) -> List[int]:
        tokens = []
        for part in self._split_text(text):
            if part in self.special_tokens:
                tokens.append(self.special_tokens[part])
            else:
                for chunk in re.findall(self.pattern, part):
                    tokens.extend(self._encode_chunk(chunk))
        return tokens

    def decode(self, tokens: List[int]) -> str:
        buffer = bytearray()
        decoded = []
        for t in tokens:
            if t in self.inverse_special:
                if buffer:
                    decoded.append(buffer.decode('utf-8', errors='replace'))
                    buffer.clear()
                decoded.append(self.inverse_special[t])
            elif t in self.vocab:
                buffer.extend(self.vocab[t])
            else:
                raise ValueError(f"Invalid token ID: {t}")
        if buffer:
            decoded.append(buffer.decode('utf-8', errors='replace'))
        return ''.join(decoded)

    def save_pretrained(self, save_directory: str):
        os.makedirs(save_directory, exist_ok=True)

        # Save merges
        with open(os.path.join(save_directory, "merges.txt"), "w") as f:
            for pair in self.merge_order:
                f.write(f"{pair[0]} {pair[1]}\n")

        # Save vocab
        vocab_data = {
            str(idx): {
                "base64": base64.b64encode(bytes_val).decode('ascii'),
                "bytes": list(bytes_val),
                "readable": self._bytes_to_readable(bytes_val)
            } for idx, bytes_val in self.vocab.items()
        }
        with open(os.path.join(save_directory, "vocab.json"), "w") as f:
            json.dump(vocab_data, f, indent=2)

        # Save config
        config = {
            "model_type": "bpe",
            "vocab_size": len(self.vocab),
            "special_tokens": self.special_tokens,
            "pattern": self.GPT4_SPLIT_PATTERN,
            "merges_count": len(self.merges)
        }
        with open(os.path.join(save_directory, "config.json"), "w") as f:
            json.dump(config, f, indent=2)

    @classmethod
    def from_pretrained(cls, save_directory: str):
        with open(os.path.join(save_directory, "config.json")) as f:
            config = json.load(f)

        tokenizer = cls(special_tokens=config["special_tokens"])

        with open(os.path.join(save_directory, "vocab.json")) as f:
            vocab_data = json.load(f)
            tokenizer.vocab = {
                int(k): base64.b64decode(v["base64"])
                for k, v in vocab_data.items()
            }

        with open(os.path.join(save_directory, "merges.txt")) as f:
            tokenizer.merge_order = []
            for line in f:
                a, b = map(int, line.strip().split())
                merge_id = 256 + len(tokenizer.merges)
                tokenizer.merges[(a, b)] = merge_id
                tokenizer.merge_order.append((a, b))

        return tokenizer

    def report_merges(self, file_path: str = None):
        """Generate merge report to console or file"""
        report = self._generate_merge_report()

        if file_path:
            self._save_merge_report(file_path, report)
            print(f"Merge report saved to: {os.path.abspath(file_path)}")
        else:
            print(report)

    def save_merge_report(self, file_path: str):
        """Save merge report to specified text file"""
        report = self._generate_merge_report()
        self._save_merge_report(file_path, report)
        print(f"Merge report saved to: {os.path.abspath(file_path)}")

    def train_from_file(self, file_path: str, vocab_size: int, encoding: str = 'utf-8'):
        """Train the tokenizer from a text file"""
        text = self._read_text_file(file_path, encoding)
        self.train(text, vocab_size)

    def encode_from_file(self, file_path: str, encoding: str = 'utf-8') -> List[int]:
        """Encode text from a file"""
        text = self._read_text_file(file_path, encoding)
        return self.encode(text)

    def _generate_merge_report(self) -> str:
        """Generate formatted merge report content"""
        report_lines = []
        if not self.merge_order:
            return "No merges performed."

        report_lines.append("Tiktoken Tokenizer Merge Report")
        report_lines.append("=" * 50)
        report_lines.append(f"Total merges performed: {len(self.merge_order)}")
        report_lines.append(f"Base vocabulary size: 256 bytes")
        report_lines.append(f"Special tokens count: {len(self.special_tokens)}")
        report_lines.append(f"Final vocabulary size: {len(self.vocab)}")
        report_lines.append("\nMerge Operations (in order of application):")

        for i, (a, b) in enumerate(self.merge_order):
            merge_id = 256 + i
            bytes_a = self.vocab.get(a, b'<UNKNOWN>')
            bytes_b = self.vocab.get(b, b'<UNKNOWN>')
            merged_bytes = self.vocab.get(merge_id, b'<UNKNOWN>')

            report_lines.append(
                f"Merge {i+1}: "
                f"{a} ({self._bytes_to_readable(bytes_a)}) + "
                f"{b} ({self._bytes_to_readable(bytes_b)}) â†’ "
                f"{merge_id} ({self._bytes_to_readable(merged_bytes)})"
            )

        return '\n'.join(report_lines)

    def _save_merge_report(self, file_path: str, content: str):
        """Internal file saving handler"""
        directory = os.path.dirname(file_path)
        if directory:
            os.makedirs(directory, exist_ok=True)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

    def _read_text_file(self, file_path: str, encoding: str = 'utf-8') -> str:
        """Read text file with error handling"""
        try:
            with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                return f.read()
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")
        except UnicodeDecodeError as e:
            raise ValueError(f"Error decoding {file_path}: {str(e)}")

    def _split_text(self, text: str) -> List[str]:
        parts = []
        remaining = text
        while remaining:
            earliest_pos = len(remaining)
            earliest_token = None

            for token in self.special_tokens:
                pos = remaining.find(token)
                if pos != -1 and pos < earliest_pos:
                    earliest_pos = pos
                    earliest_token = token

            if earliest_token is None:
                parts.append(remaining)
                break

            if earliest_pos > 0:
                parts.append(remaining[:earliest_pos])
            parts.append(earliest_token)
            remaining = remaining[earliest_pos + len(earliest_token):]
        return parts

    def _remove_special_tokens(self, text: str) -> str:
        return ''.join(part for part in self._split_text(text)
                     if part not in self.special_tokens)

    def _bytes_to_readable(self, b: bytes) -> str:
        try:
            return b.decode('utf-8')
        except UnicodeDecodeError:
            return ''.join(f'\\x{byte:02x}' for byte in b)

    def _encode_chunk(self, chunk: str) -> List[int]:
        byte_ids = list(chunk.encode('utf-8'))
        while len(byte_ids) >= 2:
            for pair in self.merge_order:
                i = 0
                while i < len(byte_ids) - 1:
                    if (byte_ids[i], byte_ids[i+1]) == pair:
                        byte_ids = byte_ids[:i] + [self.merges[pair]] + byte_ids[i+2:]
                        break
                    i += 1
                else:
                    continue
                break
            else:
                break
        return byte_ids

    def _apply_merge(self, ids: List[int], pair: Tuple[int, int], new_id: int) -> List[int]:
        new_ids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:
                new_ids.append(new_id)
                i += 2
            else:
                new_ids.append(ids[i])
                i += 1
        return new_ids